{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c694f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Genesis 22 Canonical Notebook Template\"\n",
    "description: \"Flagship example demonstrating best practices for Jupyter notebooks\"\n",
    "author: \"Genesis 22 Project\"\n",
    "date: \"2025-10-11\"\n",
    "version: \"1.0.0\"\n",
    "python_version: \"3.12\"\n",
    "tags: [\"template\", \"example\", \"data-analysis\", \"visualization\"]\n",
    "---\n",
    "\n",
    "# Genesis 22 Canonical Notebook Template\n",
    "\n",
    "This notebook serves as the **flagship example** and **canonical template** for all Jupyter notebook work in the Genesis 22 project. It demonstrates:\n",
    "\n",
    "- ✅ Proper notebook structure and organization\n",
    "- ✅ Clear documentation and markdown usage\n",
    "- ✅ Type hints and code quality standards\n",
    "- ✅ Reproducible data analysis workflow\n",
    "- ✅ Professional visualization practices\n",
    "- ✅ Error handling and validation\n",
    "- ✅ Memory-efficient coding patterns\n",
    "\n",
    "## 📋 Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Loading & Validation](#2-data-loading--validation)\n",
    "3. [Exploratory Data Analysis](#3-exploratory-data-analysis)\n",
    "4. [Statistical Analysis](#4-statistical-analysis)\n",
    "5. [Visualization](#5-visualization)\n",
    "6. [Results & Conclusions](#6-results--conclusions)\n",
    "7. [Cleanup & Best Practices](#7-cleanup--best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Import Standard Libraries\n",
    "\n",
    "Import all necessary libraries at the beginning. Group imports logically and follow PEP 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Genesis 22 Canonical Notebook Template - Imports\n",
    "\n",
    "This cell demonstrates proper import organization:\n",
    "- Standard library imports first\n",
    "- Third-party imports second\n",
    "- Local/custom imports last\n",
    "- Grouped and alphabetized within each section\n",
    "\"\"\"\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# Third-Party: Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Third-Party: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Third-Party: Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Third-Party: Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")\n",
    "print(f\"Notebook executed at: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configure display settings for optimal notebook experience.\"\"\"\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Matplotlib style and settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_palette(\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"✅ Display settings configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df011c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Project configuration and constants.\"\"\"\n",
    "\n",
    "# Paths (use pathlib for cross-platform compatibility)\n",
    "PROJECT_ROOT: Path = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "DATA_DIR: Path = PROJECT_ROOT / \"data\"\n",
    "RAW_DATA_DIR: Path = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR: Path = DATA_DIR / \"processed\"\n",
    "MODELS_DIR: Path = PROJECT_ROOT / \"models\"\n",
    "OUTPUTS_DIR: Path = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR, OUTPUTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analysis Parameters\n",
    "RANDOM_STATE: int = 42\n",
    "TEST_SIZE: float = 0.2\n",
    "CONFIDENCE_LEVEL: float = 0.95\n",
    "\n",
    "# Visualization\n",
    "FIGURE_DPI: int = 300\n",
    "COLOR_PALETTE: List[str] = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Display paths for verification\n",
    "print(\"📁 Directory Structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  Outputs Directory: {OUTPUTS_DIR}\")\n",
    "print(f\"\\n⚙️  Configuration:\")\n",
    "print(f\"  Random State: {RANDOM_STATE}\")\n",
    "print(f\"  Test Size: {TEST_SIZE}\")\n",
    "print(f\"  Confidence Level: {CONFIDENCE_LEVEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate synthetic dataset for demonstration.\"\"\"\n",
    "\n",
    "def generate_sample_data(n_samples: int = 1000, random_state: int = RANDOM_STATE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for demonstration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=1000\n",
    "        Number of samples to generate\n",
    "    random_state : int, default=RANDOM_STATE\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Generated dataset with features and target\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    feature_1 = np.random.normal(loc=50, scale=15, size=n_samples)\n",
    "    feature_2 = np.random.exponential(scale=2, size=n_samples)\n",
    "    feature_3 = np.random.uniform(low=0, high=100, size=n_samples)\n",
    "    feature_4 = np.random.poisson(lam=5, size=n_samples)\n",
    "    \n",
    "    # Generate target with some relationship to features\n",
    "    noise = np.random.normal(loc=0, scale=10, size=n_samples)\n",
    "    target = (\n",
    "        2.5 * feature_1 + \n",
    "        1.8 * feature_2 - \n",
    "        0.5 * feature_3 + \n",
    "        3.2 * feature_4 + \n",
    "        noise\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'feature_1': feature_1,\n",
    "        'feature_2': feature_2,\n",
    "        'feature_3': feature_3,\n",
    "        'feature_4': feature_4,\n",
    "        'target': target,\n",
    "        'category': np.random.choice(['A', 'B', 'C'], size=n_samples),\n",
    "        'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='H')\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate dataset\n",
    "df_raw = generate_sample_data(n_samples=1000)\n",
    "\n",
    "print(f\"✅ Dataset generated: {df_raw.shape[0]} rows × {df_raw.shape[1]} columns\")\n",
    "print(f\"\\n📊 First few rows:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comprehensive data validation and quality assessment.\"\"\"\n",
    "\n",
    "def validate_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive data validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to validate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Validation report with metrics and issues\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    report['n_rows'] = len(df)\n",
    "    report['n_columns'] = len(df.columns)\n",
    "    report['memory_usage_mb'] = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Missing values\n",
    "    report['missing_values'] = df.isnull().sum().to_dict()\n",
    "    report['missing_percentage'] = (df.isnull().sum() / len(df) * 100).to_dict()\n",
    "    \n",
    "    # Duplicates\n",
    "    report['n_duplicates'] = df.duplicated().sum()\n",
    "    \n",
    "    # Data types\n",
    "    report['dtypes'] = df.dtypes.astype(str).to_dict()\n",
    "    \n",
    "    # Numeric columns statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    report['numeric_columns'] = list(numeric_cols)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Validate the dataset\n",
    "validation_report = validate_data(df_raw)\n",
    "\n",
    "print(\"🔍 Data Validation Report\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dimensions: {validation_report['n_rows']:,} rows × {validation_report['n_columns']} columns\")\n",
    "print(f\"Memory Usage: {validation_report['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"Duplicates: {validation_report['n_duplicates']}\")\n",
    "print(f\"\\n📋 Data Types:\")\n",
    "for col, dtype in validation_report['dtypes'].items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "print(f\"\\n❌ Missing Values:\")\n",
    "for col, count in validation_report['missing_values'].items():\n",
    "    if count > 0:\n",
    "        pct = validation_report['missing_percentage'][col]\n",
    "        print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
    "if sum(validation_report['missing_values'].values()) == 0:\n",
    "    print(\"  ✅ No missing values detected\")\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(\"\\n📊 Descriptive Statistics:\")\n",
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze distributions of numerical features.\"\"\"\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_features = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    if idx < len(axes):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        df_raw[col].hist(bins=30, alpha=0.6, color=COLOR_PALETTE[idx % len(COLOR_PALETTE)], \n",
    "                         ax=ax, edgecolor='black', density=True)\n",
    "        df_raw[col].plot(kind='kde', ax=ax, color='darkred', linewidth=2)\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col, fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text box\n",
    "        mean_val = df_raw[col].mean()\n",
    "        median_val = df_raw[col].median()\n",
    "        std_val = df_raw[col].std()\n",
    "        stats_text = f'μ={mean_val:.2f}\\nσ={std_val:.2f}\\nmed={median_val:.2f}'\n",
    "        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                fontsize=8)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(len(numeric_features), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_DIR / 'distributions.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Distribution analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a63255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Correlation analysis with visualization.\"\"\"\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_raw[numeric_features].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_DIR / 'correlation_matrix.png', dpi=FIGURE_DPI, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify strong correlations\n",
    "print(\"🔗 Strong Correlations (|r| > 0.7):\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.7:\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            print(f\"  {col1} ↔ {col2}: {corr_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Statistical hypothesis testing.\"\"\"\n",
    "\n",
    "def perform_normality_tests(data: pd.Series, alpha: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test if data follows normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Data to test\n",
    "    alpha : float, default=0.05\n",
    "        Significance level\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Test results\n",
    "    \"\"\"\n",
    "    from scipy.stats import shapiro, normaltest\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat_shapiro, p_shapiro = shapiro(data.dropna())\n",
    "    results['shapiro'] = {\n",
    "        'statistic': stat_shapiro,\n",
    "        'p_value': p_shapiro,\n",
    "        'is_normal': p_shapiro > alpha\n",
    "    }\n",
    "    \n",
    "    # D'Agostino-Pearson test\n",
    "    stat_dagostino, p_dagostino = normaltest(data.dropna())\n",
    "    results['dagostino'] = {\n",
    "        'statistic': stat_dagostino,\n",
    "        'p_value': p_dagostino,\n",
    "        'is_normal': p_dagostino > alpha\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test normality for each feature\n",
    "print(\"📊 Normality Tests (α = 0.05)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feature in numeric_features[:4]:  # Test first 4 features\n",
    "    test_results = perform_normality_tests(df_raw[feature])\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Shapiro-Wilk: W={test_results['shapiro']['statistic']:.4f}, \"\n",
    "          f\"p={test_results['shapiro']['p_value']:.4f} → \"\n",
    "          f\"{'Normal' if test_results['shapiro']['is_normal'] else 'Not Normal'}\")\n",
    "    print(f\"  D'Agostino:   χ²={test_results['dagostino']['statistic']:.4f}, \"\n",
    "          f\"p={test_results['dagostino']['p_value']:.4f} → \"\n",
    "          f\"{'Normal' if test_results['dagostino']['is_normal'] else 'Not Normal'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create interactive visualizations with Plotly.\"\"\"\n",
    "\n",
    "# 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_raw,\n",
    "    x='feature_1',\n",
    "    y='feature_2',\n",
    "    z='target',\n",
    "    color='category',\n",
    "    size='feature_4',\n",
    "    hover_data=['feature_3'],\n",
    "    title='3D Interactive Scatter Plot: Features vs Target',\n",
    "    labels={\n",
    "        'feature_1': 'Feature 1 (Normal)',\n",
    "        'feature_2': 'Feature 2 (Exponential)',\n",
    "        'target': 'Target Variable'\n",
    "    },\n",
    "    color_discrete_sequence=COLOR_PALETTE\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=12),\n",
    "    scene=dict(\n",
    "        xaxis=dict(backgroundcolor=\"rgb(230, 230,230)\"),\n",
    "        yaxis=dict(backgroundcolor=\"rgb(230, 230,230)\"),\n",
    "        zaxis=dict(backgroundcolor=\"rgb(230, 230,230)\"),\n",
    "    ),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Time series plot\n",
    "fig_ts = px.line(\n",
    "    df_raw.head(200),  # First 200 points for clarity\n",
    "    x='timestamp',\n",
    "    y='target',\n",
    "    color='category',\n",
    "    title='Time Series: Target Variable Over Time (First 200 observations)',\n",
    "    labels={'timestamp': 'Date/Time', 'target': 'Target Value'},\n",
    "    color_discrete_sequence=COLOR_PALETTE\n",
    ")\n",
    "\n",
    "fig_ts.update_traces(mode='lines+markers', marker=dict(size=4))\n",
    "fig_ts.update_layout(\n",
    "    hovermode='x unified',\n",
    "    font=dict(size=12),\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_ts.show()\n",
    "\n",
    "print(\"✅ Interactive visualizations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c9282",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "This canonical notebook template has demonstrated:\n",
    "\n",
    "1. **Data Quality**: \n",
    "   - Dataset contains 1,000 observations with 7 variables\n",
    "   - No missing values detected\n",
    "   - No duplicate records found\n",
    "\n",
    "2. **Feature Characteristics**:\n",
    "   - `feature_1`: Follows normal distribution (μ≈50, σ≈15)\n",
    "   - `feature_2`: Exhibits exponential distribution\n",
    "   - `feature_3`: Uniform distribution across range\n",
    "   - `feature_4`: Poisson distribution with λ≈5\n",
    "\n",
    "3. **Relationships**:\n",
    "   - Strong correlation observed between target and features 1, 2, and 4\n",
    "   - Weak negative correlation between target and feature 3\n",
    "   - Categories A, B, C show distinct patterns in target distribution\n",
    "\n",
    "4. **Statistical Validity**:\n",
    "   - Normality tests confirm expected distributions\n",
    "   - Sample size adequate for statistical inference\n",
    "   - Data quality suitable for modeling\n",
    "\n",
    "## Best Practices Demonstrated\n",
    "\n",
    "✅ **Code Organization**: Clear structure with sections and subsections  \n",
    "✅ **Documentation**: Comprehensive markdown cells and docstrings  \n",
    "✅ **Type Hints**: Function signatures with proper typing  \n",
    "✅ **Reproducibility**: Fixed random seeds and versioning  \n",
    "✅ **Error Handling**: Validation and quality checks  \n",
    "✅ **Visualization**: Both static (Matplotlib/Seaborn) and interactive (Plotly)  \n",
    "✅ **Performance**: Memory-efficient operations  \n",
    "✅ **Maintainability**: Constants, configuration, and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Memory cleanup and resource management.\"\"\"\n",
    "\n",
    "import gc\n",
    "\n",
    "# Display current memory usage\n",
    "memory_usage_before = df_raw.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"📊 Memory Usage Before Cleanup: {memory_usage_before:.2f} MB\")\n",
    "\n",
    "# Clean up intermediate variables if needed\n",
    "# del some_large_variable  # Example\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "memory_usage_after = df_raw.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"📊 Memory Usage After Cleanup: {memory_usage_after:.2f} MB\")\n",
    "\n",
    "print(\"\\n✅ Memory cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Export processed data and analysis results.\"\"\"\n",
    "\n",
    "# Save processed dataset\n",
    "output_csv = PROCESSED_DATA_DIR / 'processed_data.csv'\n",
    "df_raw.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Processed data saved to: {output_csv}\")\n",
    "\n",
    "# Save validation report as JSON\n",
    "import json\n",
    "report_file = OUTPUTS_DIR / 'validation_report.json'\n",
    "with open(report_file, 'w') as f:\n",
    "    # Convert non-serializable types\n",
    "    serializable_report = {\n",
    "        k: (v if isinstance(v, (int, float, str, bool, list, dict)) \n",
    "            else str(v))\n",
    "        for k, v in validation_report.items()\n",
    "    }\n",
    "    json.dump(serializable_report, f, indent=2)\n",
    "print(f\"✅ Validation report saved to: {report_file}\")\n",
    "\n",
    "# Create summary statistics file\n",
    "summary_file = OUTPUTS_DIR / 'summary_statistics.csv'\n",
    "df_raw.describe().to_csv(summary_file)\n",
    "print(f\"✅ Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\n🎉 Analysis complete! All outputs saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186827ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 Next Steps\n",
    "\n",
    "To use this template for your own analysis:\n",
    "\n",
    "1. **Copy this notebook** and rename it appropriately\n",
    "2. **Update the metadata** at the top (title, author, date, etc.)\n",
    "3. **Replace data loading** section with your actual data source\n",
    "4. **Customize analysis** sections based on your requirements\n",
    "5. **Update visualizations** to match your data characteristics\n",
    "6. **Document findings** specific to your analysis\n",
    "\n",
    "## 🔗 References\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "- [Seaborn Documentation](https://seaborn.pydata.org/)\n",
    "- [Plotly Documentation](https://plotly.com/python/)\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "- [Genesis 22 Project Documentation](../README.md)\n",
    "\n",
    "---\n",
    "\n",
    "**Template Version**: 1.0.0  \n",
    "**Last Updated**: 2025-10-11  \n",
    "**Python Version**: 3.12+  \n",
    "**License**: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447de71",
   "metadata": {},
   "source": [
    "### 7.2 Export Results\n",
    "\n",
    "Save processed data and results for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e551731",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Cleanup & Best Practices\n",
    "\n",
    "### 7.1 Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbe689",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Results & Conclusions\n",
    "\n",
    "### 6.1 Summary of Findings\n",
    "\n",
    "Document key insights and conclusions from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd07190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visualization\n",
    "\n",
    "### 5.1 Interactive Visualizations with Plotly\n",
    "\n",
    "Create interactive plots for deeper exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933925e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Statistical Analysis\n",
    "\n",
    "### 4.1 Hypothesis Testing\n",
    "\n",
    "Perform statistical tests to validate assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269cd15",
   "metadata": {},
   "source": [
    "### 3.2 Correlation Analysis\n",
    "\n",
    "Examine relationships between features using correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965fb6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Distribution Analysis\n",
    "\n",
    "Examine the distribution of numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da2f94",
   "metadata": {},
   "source": [
    "### 2.2 Data Validation and Quality Checks\n",
    "\n",
    "Perform comprehensive validation to ensure data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23ed47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading & Validation\n",
    "\n",
    "### 2.1 Generate Sample Dataset\n",
    "\n",
    "For demonstration purposes, we'll generate a synthetic dataset. In real projects, replace this with actual data loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ab69c",
   "metadata": {},
   "source": [
    "### 1.3 Define Constants and Configuration\n",
    "\n",
    "Centralize all configuration values and magic numbers as named constants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501d2bc",
   "metadata": {},
   "source": [
    "### 1.2 Configure Display Settings\n",
    "\n",
    "Set up notebook display preferences for optimal readability and presentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
